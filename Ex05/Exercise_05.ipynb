{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning\n",
    "#### Machine Learning in Science, University of TÃ¼bingen, Summer Semester 2022\n",
    "## Exercise 05\n",
    "\n",
    "**hand in before 27.05.2022, 12:00 p.m. (noon)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "## a)\n",
    "\\begin{align*}\n",
    "    p(y\\mid r) &= \\frac{(r \\Delta t)^y \\exp(-r \\Delta t)}{y!} \\\\    \n",
    "    p(y_{1:T, 1:L}\\mid r) &= \\prod_{t,l}^{T,L}p(y_{t,l}\\mid r)\\\\\n",
    "    &= \\prod_{t,l}^{T,L} \\frac{(r \\Delta t)^{y_{t,l}} \\exp(-r \\Delta t)}{y_{t,l}!} \n",
    "    \\\\\n",
    "    \\log p(y_{1:T, 1:L}\\mid r) \n",
    "    &= \\sum_{t,l}^{T,L} \\log (r \\Delta t)^{y_{t,l}} -r \\Delta t - \\log y_{t,l}! \\\\\n",
    "    &= \\sum_{t,l}^{T,L} y_{t,l} \\log(r \\Delta t)-r \\Delta t - \\sum_{i=1}^{y_{t,l}}\\log i\n",
    "    \\\\\n",
    "    \\frac{\\partial \\log p(y_{1:T, 1:L}\\mid r)}{\\partial r}\n",
    "    &= \\sum_{t,l}^{T,L} \\frac{y_{t,l}\\Delta t}{r \\Delta t} - \\Delta t \\\\\n",
    "    &= - TL \\Delta t + \\frac{1}{r}\\sum_{t,l}^{T,L}  y_{t,l} \\overset{!}{=} 0  \\\\\n",
    "    r &= \\frac{1}{TL \\Delta t}\\sum_{t,l}^{T,L}  y_{t,l}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the theory exercise this week, you explored the use of Poisson Generalized Linear Models in the context of spike trains in neuronal activity. Here, you will implement and evaluate these results in practice.\n",
    "\n",
    "## Outline\n",
    "1) Implement the Poisson GLM for the neuronal activity setting, and simulate data from it.\n",
    "2) Estimate the neuron's receptive field (the filter $\\boldsymbol{\\omega}$) \n",
    "    - using the spike trains only\n",
    "    - using an MLE for $\\boldsymbol{\\omega}$\n",
    "    - using a Laplace approximation to the posterior over $\\boldsymbol{\\omega}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import factorial,gammaln\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Implement the model and generate ground-truth data.\n",
    "\n",
    "Recall the model of the spike events in the neurons from the theory exercise:\n",
    "\n",
    "\\begin{align*}\n",
    "\tz_t &= \\sum_{k=1}^K \\omega_k x_{t-k} + b \\\\ \n",
    "\tr_t &= \\exp(z_t) \\\\ \n",
    "\ty_t &= \\text{Poisson}(r_t \\Delta t).\n",
    "\\end{align*}\n",
    "\n",
    "As a first step you want to generate spike counts from this model that we treat as our \"observed\" data below. To this end, you need a stimulus $\\boldsymbol{x}$, and the underlying filter $\\boldsymbol{\\omega}$. \n",
    "Note that in a real world setting this would not be given: the filter is a property of the neuron that we want to discover, it determines what kind of stimulus it response to and how. \n",
    "\n",
    "To generate a \"ground-truth\" filter we provided `make_filter` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filter(lags, mu1=3., mu2=8., s1=1., s2=5., eta=.2):\n",
    "    \"\"\"Returns a filter constructed by a mixture of 2 Gaussians (N(range(lags) | mu1, s1) - eta * N(range(lags) | mu2, s2)).\n",
    "    Parameters:\n",
    "    lags : int. value. Length of filter to be constructed.\n",
    "    mu1 : float value. Mean of first Gaussian.\n",
    "    mu2 : float value. Mean of second Gaussian.\n",
    "    s1 : float value. Std of first Gaussian.\n",
    "    s2 : float value. Std of second Gaussian.\n",
    "    eta : float value. Weight parameter for second Gaussian\n",
    "    \n",
    "    Returns:\n",
    "    _filter : vector of float values of size 'lags'.\n",
    "    \"\"\"\n",
    "    x = np.arange(lags)\n",
    "    gauss1 = np.exp(- (x - mu1) ** 2 / (2 * s1**2))\n",
    "    gauss2 = eta * np.exp(- (x - mu2) ** 2 / (2 * s2**2))\n",
    "    _filter = gauss1 -  gauss2;\n",
    "    _filter = _filter / np.linalg.norm(_filter)\n",
    "    return _filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Along with the fixed parameters given below, generate a convolution filter $\\boldsymbol{\\omega}$. \n",
    "\n",
    "(b) Use the given stimulus to generate a firing rate and the corresponding spike trains for each trial, $\\mathbf{y_l}, \\; l=1, \\ldots, L$. This will be our ground truth dataset. (Hint: given the parameters below you should obtain a binary matrix of shape `(200, 100)` where row is a trial, and each column a time step.)\n",
    "\n",
    "(c) Plot the generated spike trains and the firing rate next below each other in two subplots. Make sure to add titles and axis labels with correct units. (Hint: to plot the spike train you may want to use `plt.imshow` with the `cmap=binary` option to show each spike as a small black dot.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stimulus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9621715c0293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Make repeats of stimulus: we use the same stimulus for evert trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mstimulus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimulus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stimulus' is not defined"
     ]
    }
   ],
   "source": [
    "# a) Make filter\n",
    "\n",
    "# Stimulus lag: length the history dependence on the stimulus\n",
    "k = 25\n",
    "# Delta t, timebin size\n",
    "dt = 0.1\n",
    "\n",
    "stimulus_length = 100\n",
    "num_trials = 200\n",
    "\n",
    "# The baseline parameter (b in the formula) we just set to zero for now.\n",
    "baseline = 0.0\n",
    "\n",
    "\n",
    "# Your code here... \n",
    "#_filter =  # you can use the default parameters of the make_filter function\n",
    "\n",
    "\n",
    "# Generate stimulus: we use a Gaussian white noise stimulus.\n",
    "#stimulus = your code here...\n",
    "\n",
    "# Make repeats of stimulus: we use the same stimulus for evert trial\n",
    "stimulus = np.tile(stimulus, num_trials).reshape(num_trials, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Generate spike trains from the Poisson model given \n",
    "\n",
    "# Your code here...\n",
    "\n",
    "\n",
    "# Simulate from model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Plot spike counts and predicted firing rate\n",
    "# Your code here...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Estimating the underlying filter\n",
    "The data above was generate from a single neuron. The neuron was given a stimulus as input and in produced a spike train, repeated for certain number of trials. \n",
    "The way the neuron processes the incoming stimulus into a spike train output can be characterized by its filter. Above we have generated this filter ourselves, now we want to rediscover it from data.\n",
    "\n",
    "We will explore different approaches to estimate the underlying filter $\\omega$: from data only, using an MLE, using the Laplace approximation.\n",
    "But for all approaches we have to apply the filter to potentially a high number of trials, which can be computationally inefficient. \n",
    "Looking at the formulation of the Poisson GLM model we might recognize that the shifted multiplication of $\\omega$ with the stimulus $x$ can be seen as convolution. \n",
    "Using this insight we can vectorize the application of the filter on the stimulus:\n",
    "\n",
    "##### Hankel Matrix\n",
    "\n",
    "The calculation of the convolution $\\boldsymbol{\\omega}*\\vec{x}$ can be vectorized using the **Hankel** matrix:\n",
    "\\begin{equation*}\n",
    "\\mathbf{X} = \\sum\\limits_{t=k}^{T} \\mathbf{x}_{t-k:t}\n",
    "\\end{equation*}\n",
    "The Hankel matrix has shape `(num_trials * stimulus_length, k)` and contains the stimulus concatenated over all trials (rows), and shifted for every stimulus time lag $k$ (columns). \n",
    "\n",
    "Using the Hankel matrix the convolution can be written as follows (see also implementation below):\n",
    "\\begin{equation*}\n",
    "z_{t} = \\mathbf{X}\\cdot \\omega + b\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) The spike triggered average (STA)\n",
    "The first approach is descriptive, i.e., we do not take into account our model but just estimate the filter from the observed spike trains. \n",
    "The idea of the spike triggered average (STA) is to estimate the filter by averaging over all stimuli that resulted in a spike. \n",
    "\n",
    "To calculate the STA you need to multiply the spike trains with the stimulus for every time lag.  \n",
    "The Hankel matrix lets you do this in vectorized form:\n",
    "\\begin{equation*}\n",
    "\\text{STA} = \\frac{\\sum_{t=k}^{T} \\mathbf{y}_t  \\mathbf{x}_{t-k\\text{ }:\\text{ }t}}{\\sum_{t=k}^{T} \\mathbf{y}_t} = \\frac{1}{\\sum_{t}^{T} \\mathbf{y}_t}\\mathbf{y}\\cdot \\mathbf{X} \n",
    "\\end{equation*}\n",
    "\n",
    "(a) Generate the spike trains $\\mathbf{y}$ using the given stimulus and filter (as above), but this time via the convolution between $\\boldsymbol{\\omega}$ and the stimulus. \n",
    "We provided a `convolution` function that you may use. (Hint: you need to generate $z$ with the `convolution` fun, then apply the non-linearity to get the rate, and then pass the rate to the Poisson process).\n",
    "\n",
    "(b) Use the Hankel matrix (see function below) to calculate the STA. \n",
    "\n",
    "(c) Plot the results: Again plot the spike trains and the firing rate in separate subplots. In an additional subplot plot the true filter and the STA estimated filter on top of each other. Make sure to add labels, titles and legends where needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hankel(lags, _input):\n",
    "    \"\"\"Returns a Hankel matrix of size _input.size x lags\n",
    "    Parameters:\n",
    "    lags : int value. Number of input timebins in the past on which the response depends.\n",
    "    _input : vector of float values. Input sequence to be converted in to a Hankel matrix.\n",
    "    \n",
    "    Returns:\n",
    "    hank : matrix of float values of size '_input.size x lags'\n",
    "    \"\"\"\n",
    "    hankel_collect = np.zeros((0,lags))\n",
    "    for inp in _input:\n",
    "        hank = np.zeros((len(inp), lags))\n",
    "        hank[:, 0] = inp\n",
    "        for lag in range(1, lags):\n",
    "            hank[lag:, lag] = inp[:-lag]\n",
    "        hankel_collect = np.concatenate((hankel_collect,hank),0)\n",
    "    return hankel_collect\n",
    "\n",
    "\n",
    "def convolution(_filter, _input):\n",
    "    \"\"\"Returns the convolution product of the inputs.\n",
    "    Parameters:\n",
    "    _filter : vector of float values containing the convolutional filter.\n",
    "    _input : int value. The input to be convolved with the filter.\n",
    "    \n",
    "    Returns:\n",
    "    Vector of float values, of same shape as '_input'.\n",
    "    \"\"\"\n",
    "    hank = hankel(len(_filter), _input)\n",
    "    return np.dot(hank, _filter).reshape(_input.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Your code goes here..,\n",
    "\n",
    "# Calculate firing rate using convolution (via Hankel matrix)\n",
    "\n",
    "\n",
    "# Simulate from model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Your code goes here...\n",
    "\n",
    "# Calculate STA via the Hankel matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Plot spike counts and predicted firing rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Maximum Likelihood Estimate\n",
    "\n",
    "Above we estimated the filter from data. But we might be able to obtain a better fit if we take into account our generative model of the data---the Poisson GLM. \n",
    "Let's see if we can learn the parameters of this model: We get the MLE of the filter $\\boldsymbol{\\omega}$ using numerical optimization (there is no closed form, see exercise sheet). \n",
    "Because we set up our model as a GLM, the likelihood is guaranteed to be concave so that there exist a single optimum. \n",
    "Using the results from the theory exercises:\n",
    "\n",
    "(b) In the exericse sheet you obtained the Laplace  to obtain the posterior standard deviation.  \n",
    "\n",
    "(a) Write a function to calculate the (negative) log likelihood, $p(\\mathbf{x},\\mathbf{y}|\\mathbf{\\omega},b)$. **HINT**: Certain common helper functions, e.g. the factorial, can be imported rather than implemented from scratch.\n",
    "\n",
    "(b) Using `scipy.optimize.minimize` or otherwise, identify the MLE.\n",
    "\n",
    "(c) Plot the result: show the true filter, the STA estimate and the MLE estimate on top of each. Add labels, legend, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summed_factorial(responses, log = True):\n",
    "    \"\"\" sum up the factorial of  a vector or matrix \n",
    "        set logarithm flag\n",
    "    \"\"\"\n",
    "    responses = np.array(responses)\n",
    "    responses = responses.flatten()\n",
    "    responses = factorial(responses)\n",
    "    if log:\n",
    "        responses = np.log(responses)\n",
    "    return np.sum(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Write down a Python function that takes as input the LNP parameters (filter and baseline),\n",
    "# the groundtruth stimulus and the groundtruth responses, and returns the log likelihood of the model.\n",
    "\n",
    "# Your code goes here:\n",
    "\n",
    "def log_likelihood(params, stimulus, responses, dt = 1):\n",
    "    \"\"\" log likelihood of the model    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def neg_log_likelihood(params, stimulus, responses, dt = 1):\n",
    "    \"\"\" return negative log likelihood\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Minimize your log likelihood to get the MLE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Plot the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the MLE estimate is already much better in comparison to the STA estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Maximum A-Posteriori Estimate\n",
    "\n",
    "As in final part of the exercise sheet we now want to incorporate prior knowledge about the shape of the filter and perform Bayesian inference. \n",
    "Let us use the same prior as in the exercise sheet on the filter (standard normal). \n",
    "Like for the MLE, the MAP for the Poisson GLM we are using here is not available in analytical form, but it is concave, so we can get it with numerical optimization. \n",
    "\n",
    "(a) Write a function to calculate the (negative) log posterior, $p(\\boldsymbol{\\omega},b|\\mathbf{x},\\mathbf{y})$. **HINT**: log posterior = log prior + log likelihood\n",
    "\n",
    "(b) Get the MAP by numerical optimization. \n",
    "\n",
    "(c) Use the Laplace approximation (see exercise sheet) and your results from the exercise sheet to write a function that returns the Hessian. Obtain the approximate posterior standard deviation around the MAP. \n",
    "\n",
    "(d) Plot the results: add a plot showing the true filter, the STA, the MLE and the MAP with error bars (marginal Laplace posterior standard deviation). Add labels, titles and legend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Write down a Python function that takes as input the LNP parameters (filter and baseline),\n",
    "# the groundtruth stimulus and the groundtruth responses, and returns the log posterior of the model according\n",
    "# to the Laplace approximation.\n",
    "\n",
    "def log_posterior(params, stimulus, response, dt):\n",
    "    \"\"\"Returns log posterior of a linear-nonlinear Poisson model.\n",
    "    Parameters:\n",
    "    params : vector of floats containing parameters of model. Last element of vector should be the baseline offset.\n",
    "    stimulus : vector / matrix of floats containing the input stimulus.\n",
    "    response : vector / matrix of floats containing the responses of the neuron to the stimulus. Should be of same shape as the stimulus.\n",
    "    dt : float value containing size of timebins in s.\n",
    "    \n",
    "    Returns:\n",
    "    log_posterior : float value.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def nlogpost(params, stimulus, response, dt):\n",
    "    \"\"\"Returns negative log posterior of a linear-nonlinear Poisson model.\n",
    "    Parameters:\n",
    "    params : vector of floats containing parameters of model. Last element of vector should be the baseline offset.\n",
    "    stimulus : vector / matrix of floats containing the input stimulus.\n",
    "    response : vector / matrix of floats containing the responses of the neuron to the stimulus. Should be of same shape as the stimulus.\n",
    "    dt : float value containing size of timebins in s.\n",
    "    \n",
    "    Returns:\n",
    "    negative log posterior : float value\n",
    "    \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Find and plot the MAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Write down a Python function that takes as input the LNP parameters (filter and baseline),\n",
    "# the groundtruth stimulus and the groundtruth responses, and returns the Hessian matrix of \n",
    "# the true posterior distribution of the filter and baseline parameter.\n",
    "\n",
    "def hessian(params, stimulus, response, dt):\n",
    "    \"\"\"Returns the inverse of the Hessian of the log posterior of a linear nonlinear Poisson model, assuming a multivariate Gaussian prior with identity covariance matrix.\n",
    "    Parameters:\n",
    "    params : vector of floats containing parameters of model. Last element of vector should be the baseline offset.\n",
    "    stimulus : vector / matrix of floats containing the input stimulus.\n",
    "    response : vector / matrix of floats containing the responses of the neuron to the stimulus. Should be of same shape as the stimulus.\n",
    "    dt : float value containing size of timebins in s.\n",
    "    \n",
    "    Returns:\n",
    "    H_filter : matrix of size K x K, where K is the length of the convolutional filter.\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) Plot the MAP estimate for the filter along with confidence intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the MAP estimate lies close to the MLE estimate, but that it additionally provides error bars that represent the marginal standard deviation of the posterior, approximated with a Laplace approximation. \n",
    "This provides with uncertainty estimates on the filter weights. \n",
    "\n",
    "Overall, we can use this approach to investigate the properties of individual neurons, by estimating their stimulus filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ef9b53a5ce850816b9705a866e49207a37a04a71269aa157d9f9ab944ea42bf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

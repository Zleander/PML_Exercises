{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Machine Learning\n",
    "#### Machine Learning in Science, University of TÃ¼bingen, Summer Semester 2022\n",
    "## Exercise 11\n",
    "\n",
    "Authors: Janne Lappalainen, Jaivardhan Kapoor\n",
    "\n",
    "**hand in before 15.07.2022, 12:00 p.m. (noon)**\n",
    "\n",
    "---\n",
    "In the lecture you learned about normalizing flows, a series of bijective transformations on random variables to learn probability densities. In this programming exercise, you will explore how to build and train normalizing flows in `Pytorch`. For this assigmnent, you will use `Pyro`, a popular (and powerful) probabilistic programming library based on `Pytorch`.\n",
    "\n",
    "## Outline\n",
    "1) Creating composable and invertible transformations in `Pyro`.\n",
    "\n",
    "2) Approximating complex univariate distributions using learnable bijections in `Pyro`.\n",
    "\n",
    "3) Approximating multivariate distributions using normalizing flows -- analysing the importance of coupling flows as opposed to merely learning marginals. \n",
    "\n",
    "### References:\n",
    "\n",
    "[1] Kevin P Murphy. \"Probabilistic Machine Learning: Advanced Topics\". Chapter 23. https://probml.github.io/pml-book/book2.html\n",
    "\n",
    "[2] https://pyro.ai/examples/normalizing_flows_i.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats.distributions import beta\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "# to ignore seaborn deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# importing pyro modules \n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing flows\n",
    "## Convert a simple probability distribution to a complex probability distribution\n",
    "\n",
    "Consider a random variable $\\mathbf{u} \\in \\mathbb{R}^d$ from a simple base distribution $p_{\\mathbf{u}}(\\mathbf{u})$, e.g. standard normal -- so that we can sample it easily, $\\mathbf{u} \\sim \\mathcal{N}(0, \\mathbb{1})$. A normalizing flow is a bijective (in fact diffeomorphic, i.e. bijective and differentiable) transformation that transforms $\\mathbf{u}$ into  $\\mathbf{x}$ with a more complex distribution over it.\n",
    "\n",
    "$$\\mathbf{x} = f(\\mathbf{u}) \\qquad \\text{with }\\,\\mathbf{x} \\in \\mathbb{R}^d,\\,\\mathbf{u} \\sim p_{\\mathbf{u}}(\\mathbf{u})\\text{.}$$\n",
    "\n",
    "To sample from the distribution over $\\mathbf{x}$, denoted by $p_\\mathbf{x}(\\mathbf{x})$, we simply sample from $p_{\\mathbf{u}}(\\mathbf{u})$ and then compute $\\mathbf{x}=f(\\mathbf{u})$.\n",
    "Because $f$ is bijective, we can compute the density $p(\\mathbf{x})$ using the change-of-variables formula for random variables and the inverse function theorem for Jacobian matrices:\n",
    "\n",
    "$$p_\\mathbf{x}(\\mathbf{x}) = p_\\mathbf{u}(f^{-1}(\\mathbf{x}))|\\text{det}\\,\\mathbf{J}_{f^{-1}}(\\mathbf{y})| = p_\\mathbf{u}(\\mathbf{u})|\\text{det}\\,\\mathbf{J}_f(\\mathbf{u})|^{-1}\\text{.}$$\n",
    "\n",
    "$\\mathbf{J}_f$ with $J_{f,ij} = \\frac{\\partial f_i}{\\partial \\mathbf{u}_j}$ is the Jacobian of $f$ evaluated at $\\mathbf{u}$. We can take the logarithm of the above equation:\n",
    "\n",
    "$$\\log p_\\mathbf{x}(\\mathbf{x}) = \\log p_\\mathbf{u}(\\mathbf{u}) - \\log|\\text{det}\\,\\mathbf{J}_f(\\mathbf{u})|$$\n",
    "\n",
    "to gain an intuitive understanding: the density of $\\mathbf{x}$ is equal to the density at the corresponding point $\\mathbf{u}$ up to a term that corrects for the _warp_ in the volume caused by $f$.\n",
    "\n",
    "Furthermore, we can compose bijective transformations to produce even more expressive transformations. With $N$ transformations $f_1, \\ldots, f_N$, the initial random vector gets transformed by composing these transformations:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}^{(0)} &= \\mathbf{u}\\\\\n",
    "\\mathbf{x}^{(1)} &= f_1(\\mathbf{x}^{(0)})\\\\\n",
    "& \\vdots \\\\\n",
    "\\mathbf{x} = \\mathbf{x}^{(N)} &= f_N(\\mathbf{x}^{(N-1)})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "the density of the final transformation can be written as\n",
    "\n",
    "$$\\log p_\\mathbf{x}(\\mathbf{x}) = \\log p_\\mathbf{u}( (f_n\\circ ...\\circ f_1)^{-1} (\\mathbf{x})) - \\sum_{n=1}^N\\log|\\text{det}\\,\\mathbf{J}_{f_i}(\\mathbf{x}^{(i-1)})|,$$\n",
    "\n",
    "as the composition of invertible functions is also invertible, with the resulting Jacobian being the product of the Jacobians of all transformations. The transformation $f_n\\circ ...\\circ f_1$ desribes a succesive warp of the volume in the space of $\\mathbf{u}$. Such a family of diffeomorpishms is called \"flow\" in mathematics -- with the base density $\\log p_\\mathbf{u}$ usually being a normal distribution, the term _normalizing flows_ was coined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a composition of simple invertible transformations in Pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this programming exercise, please keep the `Pyro` [documentation](https://docs.pyro.ai/en/stable/) open and refer to respective sections for transformations, distributions, learning methods etc.\n",
    "\n",
    "1. The [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution) is a continuous probability distribution frequenty used in sciences and economics. The logarithm of a log-normally distributed variable $x$ is normally distributed, that is, $\\log(x)\\sim \\mathcal{N}(\\mu, \\sigma^2)$.\\\n",
    "\\\n",
    "Your task is to generate samples of a log-normally distributed random variable. Starting from a normally-distributed variable, compose _affine_ and _non-linear_ transformations to create a log-normal distribution with $\\mu=3, \\sigma=0.5$. Plot samples and [KDE plot](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) for both the original normal distribution and the lognormal distribution.\\\n",
    "\\\n",
    "Use `pyro.distributions` and `pyro.distributions.transforms` in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distribution by composing transforms and applying them to a base distribution\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the KDE plot of the base distribution and transformed distribution here.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing learnable invertible transformations (with the Pytorch backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In this section you will create a learnable normalizing flow and infer its parameters, given a dataset. Then you will plot samples from the flow and compare with the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Generate the dataset by sampling from the previous lognormal distribution with $\\mu=3, \\sigma=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset from previously created lognormal distribution object\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural Spline Flows](https://arxiv.org/abs/1906.04032) are a highly flexible class of learnable bijective transformations. _Monotonic rational splines_ are flexible functions that are of the form \n",
    "\n",
    "\\begin{align*}\n",
    "s(u) = \\begin{cases}\n",
    "u & u \\leq -B;\\\\\n",
    "\\frac{g_i(u)}{h_i(u)} & u \\in [b_i, b_{i+1}]~\\forall i \\in \\{1, \\ldots, K\\};\\\\\n",
    "u & u \\geq B;\n",
    " & \\end{cases}\n",
    "\\end{align*}\n",
    "The spline is made up of segments, with each segment represented by a rational function ($g,h$ are both linear or quadratic in $u$). At the knots $b_i$, i.e., the boundaries of the segments, a continuity constraint is placed on the values and derivatives:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{g_{i-1}(b_{i})}{h_{i-1}(b_{i})} &= \\frac{g_{i}(b_{i})}{h_{i}(b_{i})}, \\text{and}\\\\\n",
    "\\frac{d}{du}\\left(\\frac{g_{i-1}(b_{i})}{h_{i-1}(b_{i})}\\right) &= \\frac{d}{du}\\left(\\frac{g_{i}(b_{i})}{h_{i}(b_{i})}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "![Spline](spline.jpg \"Example of a monotonic rational spline\") ![DetSpline](splinedet.jpg \"Derivative of the Spline\")\n",
    "\n",
    "Furthermore, the parameters of $g_i, h_i$ are also constructed in a way that the spline $s(u)$ is monotonically increasing in $u$, and $s(u) = u$ at $-B, B$. This makes $s(u)$ a flexible bijection -- by learning the parameters of $g_i$'s and $h_i$'s, $p_u(u)$ can be transformed into the desired $p_x(x)$.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "(b) Scale the dataset you just created to zero mean and unit variance. Fit a learnable rational linear spline with 7 segments to the dataset. Use the loss based on Maximum Likelihood Estimation using the log-probability method of the [`Spline`](https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.transforms.Spline) object, and optimize it using `SGD`/`Adam` optimizer from [`Pyro.optim.pytorch_optimizers`](https://docs.pyro.ai/en/stable/optimization.html#pyro.optim.pytorch_optimizers).\\\n",
    "\\\n",
    "  After learning the paramters of the `Spline` object, generate new samples from it and compare the learned distribution with the original lognormal distribution. (Make sure to apply the inverse scaling you applied to the dataset in the first place for training.)\\\n",
    "\\\n",
    "  How does the learned distribution change as you increase the number of segments in the spline? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale your data, so that it falls roughly into the domain/range of the Spline object\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your distribution with learnable transformation parameters\n",
    "\n",
    "# create base distribution\n",
    "base_dist = ... \n",
    "\n",
    "# create spline transform\n",
    "# Tip: use the visualize_spline() function at the bottom of this notebook to develop intuition for the spline transformation.\n",
    "spline_transform = T.Spline(1, count_bins=7, order='linear')\n",
    "\n",
    "# create transformed distribution\n",
    "flow_dist = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the training loop after initializing the optimizer and use the loss function provided by `log_prob` method of the TransformedDistribution object. Learn the parameters by optimizing the loss.\n",
    "\n",
    "\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the base distribution, distribution of the original dataset, and samples from the learned distribution.\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) To gauge the expressivity of the rational monotonic splines, we will attempt to approximate a multimodal univariate distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_univariate_random_variable(n_samples):\n",
    "    # cheap way to create a more complicated\n",
    "    # toy univariate random variable    \n",
    "    X, y = datasets.make_circles(n_samples, noise=0.05, factor=0.5)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return X[:, 0].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 3000\n",
    "x = complex_univariate_random_variable(n_samples)\n",
    "sns.distplot(x, hist=True, kde=True, bins=40)\n",
    "plt.title(\"Complex univariate random variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (i) Create a simple spline with few (2-5) segments, and approximate the above distribution. How good is the approximation compared to a more expressive spline with 6+ segments?\\\n",
    "\\\n",
    "  (ii) Now create multiple splines with few (2-5) segments, and compose them together to make a more powerful approximation. How well does it learn to approximate this multimodal density?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the learnable TransformedDistribution object.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the parameters of the flow. Note that you may have to tweak learning hyperparameters such as number of trainig steps and learning rate.\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples from the learned flow along with the dataset, and compare the quality of the approximation.\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your observations here on the number of segments used vs the number of splines composed together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating distributions for multivariate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monotonic rational splines used in the previous sections are univariate functions. To approximate multivariate densities, we need to create expressive _multivariate bijections_.\n",
    "\n",
    "In the lecture on normalizing flows, we saw how _coupling transformations_ can be used to convert univariate pointwise bijections into more powerful multivariate bijections.\n",
    "\n",
    "In this section, you will play around with approximating a density in 2 dimensions. The dataset that will be used is a superposition of 2 hearts, as seen below.\n",
    "\n",
    "You will first approximate the density by approximating marginals using a spline for each marginal. Then you will use [spline coupling](https://docs.pyro.ai/en/stable/distributions.html#splinecoupling) transforms ([1], Section 2.1) to approximate the 2D distribution.\n",
    "\n",
    "[1] [Neural Spline Flows (NeurIPS 2019)](https://arxiv.org/pdf/1906.04032.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hearts(\n",
    "    n_samples=100, *, shuffle=True, noise=None, random_state=None, factor=0.8, offset=None,\n",
    "    scale=True\n",
    "):\n",
    "    \"\"\"Make a large heart containing a smaller heart in 2d.\n",
    "\n",
    "    A simple toy dataset to visualize clustering and classification\n",
    "    algorithms.\n",
    "\n",
    "    Read more in the :ref:`User Guide <sample_generators>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int or tuple of shape (2,), dtype=int, default=100\n",
    "        If int, it is the total number of points generated.\n",
    "        For odd numbers, the inner heart will have one point more than the\n",
    "        outer heart.\n",
    "        If two-element tuple, number of points in outer heart and inner\n",
    "        heart.\n",
    "\n",
    "        .. versionchanged:: 0.23\n",
    "           Added two-element tuple.\n",
    "\n",
    "    shuffle : bool, default=True\n",
    "        Whether to shuffle the samples.\n",
    "\n",
    "    noise : float, default=None\n",
    "        Standard deviation of Gaussian noise added to the data.\n",
    "\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Determines random number generation for dataset shuffling and noise.\n",
    "        Pass an int for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "\n",
    "    factor : float, default=.8\n",
    "        Scale factor between inner and outer heart in the range `(0, 1)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray of shape (n_samples, 2)\n",
    "        The generated samples.\n",
    "\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        The integer labels (0 or 1) for class membership of each sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    import numbers\n",
    "    from sklearn.utils import shuffle as util_shuffle\n",
    "    from sklearn.utils import check_random_state\n",
    "\n",
    "    if factor >= 1 or factor < 0:\n",
    "        raise ValueError(\"'factor' has to be between 0 and 1.\")\n",
    "\n",
    "    if isinstance(n_samples, numbers.Integral):\n",
    "        n_samples_out = n_samples // 2\n",
    "        n_samples_in = n_samples - n_samples_out\n",
    "    else:\n",
    "        try:\n",
    "            n_samples_out, n_samples_in = n_samples\n",
    "        except ValueError as e:\n",
    "            raise ValueError(\n",
    "                \"`n_samples` can be either an int or a two-element tuple.\"\n",
    "            ) from e\n",
    "\n",
    "    generator = check_random_state(random_state)\n",
    "    # so as not to have the first point = last point, we set endpoint=False\n",
    "    linspace_out = np.linspace(0, 2 * np.pi, n_samples_out, endpoint=False)\n",
    "    linspace_in = np.linspace(0, 2 * np.pi, n_samples_in, endpoint=False)\n",
    "    \n",
    "    def heart(t):\n",
    "        return (16 * np.sin(t) ** 3,\n",
    "                13 * np.cos(t) - 5 * np.cos(2 * t) - 2 * np.cos(3 * t) - np.cos(4 * t))\n",
    "    \n",
    "    outer_heart_x, outer_heart_y = heart(linspace_in)\n",
    "    inner_heart_x, inner_heart_y = heart(linspace_out)\n",
    "    if offset is not None:\n",
    "        inner_heart_x += offset[0]\n",
    "        inner_heart_y += offset[1]\n",
    "    inner_heart_x *= factor\n",
    "    inner_heart_y *= factor\n",
    "\n",
    "    X = np.vstack(\n",
    "        [np.append(outer_heart_x, inner_heart_x), np.append(outer_heart_y, inner_heart_y)]\n",
    "    ).T\n",
    "    y = np.hstack(\n",
    "        [np.zeros(n_samples_out, dtype=np.intp), np.ones(n_samples_in, dtype=np.intp)]\n",
    "    )\n",
    "    if shuffle:\n",
    "        X, y = util_shuffle(X, y, random_state=generator)\n",
    "\n",
    "    if noise is not None:\n",
    "        X += generator.normal(scale=noise, size=X.shape)\n",
    "        \n",
    "    if scale:\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the hearts dataset and visualizing marginals and joint density\n",
    "\n",
    "\n",
    "X, _ = make_hearts(n_samples, factor=0.5, noise=0.5, offset=[7, -7])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(X[:,0], label='data')\n",
    "plt.title(r'$p(x_1)$')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(X[:,1], label='data')\n",
    "plt.title(r'$p(x_2)$')\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title(r'Joint Distribution')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.scatter(X[:,0], X[:,1], label='data', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use a 2D `Spline` to learn the multivariate transformation. Plot the marginals and plot the joint distribution of the learned distribution, and compare against the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the learnable distribution with 2D Spline\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the parameters of the Spline as in the previous sections\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the marginals and joint density of the learned distribution and compare against the true data distribution.\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How well does the learned distribution approximate the marginals? How well does it approximate the joint density? Justify your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use [`SplineCoupling`](https://docs.pyro.ai/en/stable/distributions.html#splinecoupling) to learn the complex multivariate transformation. Plot and inspect the marginals and joint density.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learnable distribution\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the parameters of the coupling splie flow\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the marginals and joint of the learned distribution against the true data distribution.\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What is the quality of the marginal and joint densities with the coupling transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_spline(spline_: T.Spline, dim=0):\n",
    "    '''\n",
    "    Helper function for plotting the spline function with its derivatives.\n",
    "    Args:\n",
    "        spline_: `Spline` object\n",
    "        dim: dimension of the spline to visualize, for multivariate spline\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        D = spline_.input_dim\n",
    "        B = spline.bound\n",
    "        u = torch.from_numpy(np.linspace(-1.2*B, 1.2*B, 1000))\n",
    "        x, logdet = spline_.spline_op(u.unsqueeze(1).repeat(1, D))\n",
    "        print(x.shape, logdet.shape)\n",
    "        det = logdet.exp()\n",
    "        fig = plt.figure(figsize=(5,3), dpi=100)\n",
    "\n",
    "        plt.plot(u, x[:,dim].cpu(), color='maroon')\n",
    "        plt.xticks([-1*B, 1*B], [r'-B', r'+B'])\n",
    "        plt.yticks([-1*B, 1*B], [r'-B', r'+B'])\n",
    "        plt.xlim((-1.2*B, 1.2*B))\n",
    "        plt.ylim((-1.2*B, 1.2*B))\n",
    "        plt.axvline(x=-1*B, ymin=0, ymax=0.2/2.4, color='k', alpha=0.4, linestyle='--')\n",
    "        plt.axhline(y=-1*B, xmin=0, xmax=0.2/2.4, color='k', alpha=0.4, linestyle='--')\n",
    "        plt.axvline(x=1*B, ymax=2.2/2.4, ymin=0, color='k', alpha=0.4, linestyle='--')\n",
    "        plt.axhline(y=1*B, xmax=2.2/2.4, xmin=0, color='k', alpha=0.4, linestyle='--')\n",
    "        plt.xlabel(r'$u$')\n",
    "        plt.ylabel(r'$x=s(u)$')\n",
    "        plt.title(r'Spline function')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        fig = plt.figure(figsize=(5,3), dpi=100)\n",
    "\n",
    "        plt.plot(u, det[:,dim].cpu(), color='maroon')\n",
    "        plt.xticks([-1*B, 1*B], [r'-B', r'+B'])\n",
    "        plt.yticks([0], [r'0'])\n",
    "        plt.xlim((-1.2*B, 1.2*B))\n",
    "        plt.xlabel(r'$u$')\n",
    "        plt.ylabel(r'$\\frac{d}{du}s(u)$')\n",
    "        plt.title(r'Derivative of the spline')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "spline = T.Spline(1, count_bins=7, bound=2.0, order='quadratic')\n",
    "visualize_spline(spline)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('3.9.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8711c2a921205645d613ec0ac724630efe88b7e74d52cd047fe01837c51cfe07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
